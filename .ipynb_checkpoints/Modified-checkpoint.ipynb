{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================Load data=========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../dataset.xlsx\", header=None)\n",
    "df = df.drop([0,2,3,5,6,7,8,9,10,11,12,13,14], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.iloc[:300,:]\n",
    "train_df = df.iloc[301:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel('../dataset.xlsx') #Excelden oku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetColumn = list(df2['TANILAR'])\n",
    "targetCorpus = []\n",
    "for i in targetColumn:\n",
    "    row = i.split(\", \")\n",
    "    for c in row:\n",
    "        if c not in targetCorpus:\n",
    "            targetCorpus.append(c)\n",
    "targetCorpus = np.array(targetCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = []\n",
    "for i in targetColumn:\n",
    "    row = i.split(\", \")\n",
    "    actualRow=[]\n",
    "    for q in targetCorpus:\n",
    "        if q not in row:\n",
    "            actualRow.append(0.0)\n",
    "        else:\n",
    "             actualRow.append(1.0)\n",
    "    y_data.append(actualRow)\n",
    "y_data = np.array(y_data)\n",
    "y_test = y_data[:300,:]\n",
    "y_train = y_data[301:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to lower case\n",
    "train_texts = train_df[4].values\n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "\n",
    "test_texts = test_df[4].values\n",
    "test_texts = [s.lower() for s in test_texts]\n",
    "test_texts = test_texts[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================Convert string to index================\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "# If we already have a character list, then replace the tk.word_index\n",
    "# If not, just skip below part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------Skip part start--------------------------\n",
    "# construct a new vocabulary\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "# -----------------------Skip part end----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to index\n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "test_texts = tk.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=1014, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=1014, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>300</td>\n",
       "      <td>idrar yaparken yanma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>301</td>\n",
       "      <td>bulantı ishal halsizlik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>302</td>\n",
       "      <td>karın ağrısı üst kısımlarda karında şişkinlik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>303</td>\n",
       "      <td>midede yanma göğüste ağrı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>304</td>\n",
       "      <td>halsizlik vücutta kolay morarma eklem ağrısı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>1612</td>\n",
       "      <td>1 hafta önce seyahat sirasinda ishal idrar tut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>1613</td>\n",
       "      <td>kusma bulantı karın ağrısı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>1614</td>\n",
       "      <td>-ishal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1615</td>\n",
       "      <td>halsizlik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>1616</td>\n",
       "      <td>halsizlik yorgunluk dilinde değişiklik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1317 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1                                                  4\n",
       "301    300                               idrar yaparken yanma\n",
       "302    301                            bulantı ishal halsizlik\n",
       "303    302  karın ağrısı üst kısımlarda karında şişkinlik ...\n",
       "304    303                          midede yanma göğüste ağrı\n",
       "305    304       halsizlik vücutta kolay morarma eklem ağrısı\n",
       "...    ...                                                ...\n",
       "1613  1612  1 hafta önce seyahat sirasinda ishal idrar tut...\n",
       "1614  1613                         kusma bulantı karın ağrısı\n",
       "1615  1614                                             -ishal\n",
       "1616  1615                                          halsizlik\n",
       "1617  1616             halsizlik yorgunluk dilinde değişiklik\n",
       "\n",
       "[1317 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "train_classes = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_classes = y_train\n",
    "test_classes = y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================Char CNN=======================\n",
    "# parameter\n",
    "input_size = 1014\n",
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 69\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = len(targetCorpus)\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding weights\n",
    "embedding_weights = []  # (70, 69)\n",
    "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
    "\n",
    "for char, i in tk.word_index.items():  # from index 1 to 69\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i - 1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print('Load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding layer Initialization\n",
    "embedding_layer = Embedding(vocab_size + 1,\n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])\n",
    "\n",
    "# Model Construction\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
    "# Embedding\n",
    "x = embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
    "x = Flatten()(x)  # (None, 8704)\n",
    "# Fully connected layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1000 training samples and 100 testing samples\n",
    "# indices = np.arange(train_data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "#\n",
    "# x_train = train_data[indices][:1000]\n",
    "# y_train = train_classes[indices][:1000]\n",
    "#\n",
    "# x_test = test_data[:100]\n",
    "# y_test = test_classes[:100]\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train = train_data[indices]\n",
    "y_train = train_classes[indices]\n",
    "\n",
    "x_test = test_data\n",
    "y_test = test_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
