{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_wc_embd import WordCharEmbd\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "sentences = [\n",
    "    ['All','i','want','to','fuck','shit'],\n",
    "    ['I','am','Pretty','Good'],\n",
    "    ['fuck','this','shit'],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel('../dataset.xlsx') #Excelden oku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetColumn = list(df2['TANILAR'])\n",
    "targetCorpus = []\n",
    "for i in targetColumn:\n",
    "    row = i.split(\", \")\n",
    "    for c in row:\n",
    "        if c not in targetCorpus:\n",
    "            targetCorpus.append(c)\n",
    "targetCorpus = np.array(targetCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1617"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = []\n",
    "for i in targetColumn:\n",
    "    row = i.split(\", \")\n",
    "    actualRow=[]\n",
    "    for q in targetCorpus:\n",
    "        if q not in row:\n",
    "            actualRow.append(0.0)\n",
    "        else:\n",
    "             actualRow.append(1.0)\n",
    "    y_data.append(actualRow)\n",
    "y_data = np.array(y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1617"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../dataset.xlsx') #Excelden oku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sikayet = df['SIKAYET'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for cumle in sikayet:\n",
    "    sentences.append(cumle.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_embd = WordCharEmbd(\n",
    "    word_min_freq=0,\n",
    "    char_min_freq=0,\n",
    "    word_ignore_case=False,\n",
    "    char_ignore_case=False,\n",
    ")\n",
    "for sentence in sentences:\n",
    "    wc_embd.update_dicts(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_embd.get_batch_input(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Char (InputLayer)         [(None, None, 19)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_Word (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Char_Pre (Embedding)  (None, None, 19, 30) 2010        Input_Char[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Word (Embedding)      (None, None, 300)    626700      Input_Word[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Char (TimeDistributed (None, None, 300)    217200      Embedding_Char_Pre[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Concatenate)         (None, None, 600)    0           Embedding_Word[0][0]             \n",
      "                                                                 Embedding_Char[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, 5)            12120       Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Softmax (Dense)                 (None, 394)          2364        LSTM[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 860,394\n",
      "Trainable params: 860,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs, embd_layer = wc_embd.get_embedding_layer()\n",
    "lstm_layer = keras.layers.LSTM(units=5, name='LSTM')(embd_layer)\n",
    "softmax_layer = keras.layers.Dense(units=len(y_data[0]), activation='softmax', name='Softmax')(lstm_layer)\n",
    "model = keras.models.Model(inputs=inputs, outputs=softmax_layer)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    metrics=[keras.metrics.categorical_accuracy],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def batch_generator():\n",
    "    while True:\n",
    "        yield wc_embd.get_batch_input(sentences), np.asarray(y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    wc_embd.get_batch_input(sentences),\n",
    "   [np.asarray(y_data)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/200 [..............................] - ETA: 3:18:06 - loss: 13.5926 - categorical_accuracy: 0.0239"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generator=batch_generator(),\n",
    "    steps_per_epoch=200,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49338186, 0.50661814],\n",
       "       [0.49584606, 0.50415397],\n",
       "       [0.4919908 , 0.50800914],\n",
       "       [0.501754  , 0.49824604]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\n",
    "   wc_embd.get_batch_input(['firat','dogan','shit','Good']), batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10,\n",
    "    workers=1, use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_embd.get_word_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_embd.get_dicts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
